{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Mining for Confidence Level Detection**\n",
        "\n",
        "Lydia Lonzarich and Katie Park\n",
        "\n",
        "CPSC 322-01, Fall 2025"
      ],
      "metadata": {
        "id": "DW82ohQvOIdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "xikswvuLOWVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import mysklearn.myclassifiers\n",
        "importlib.reload(mysklearn.myclassifiers)\n",
        "from mysklearn.myclassifiers import MyRandomForestClassifier, MyDecisionTreeClassifier, MyNaiveBayesClassifier, MyDecisionTreeSolo\n",
        "\n",
        "import mysklearn.myutils\n",
        "importlib.reload(mysklearn.myutils)\n",
        "import mysklearn.myutils as myutils\n",
        "\n",
        "import mysklearn.myevaluation\n",
        "importlib.reload(mysklearn.myevaluation)\n",
        "import mysklearn.myevaluation as myevaluation\n",
        "\n",
        "import mysklearn.mypytable\n",
        "importlib.reload(mysklearn.mypytable)\n",
        "from mysklearn.mypytable import MyPyTable\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TiuuZJTxOYtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The dataset used for this project is called \"Confidence Detection Dataset,\" from Kaggle: https://www.kaggle.com/datasets/muhammadkhubaibahmad/confidence-detection-dataset\n",
        "\n",
        "</br>\n",
        "\n",
        "The dataset contains features extracted from human body landmarks and postures to classify confidence levels, which is also what is being classified for this project.\n"
      ],
      "metadata": {
        "id": "7RT_43R0Od2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Findings\n",
        "\n",
        "3 different classifiers were used on the dataset:\n",
        "1. Random Forest Classifier\n",
        "1. Decision Tree Classifier\n",
        "1. Naive Bayes Classifier\n",
        "\n",
        "After using all 3 classifiers on the dataset, the classifier that performed the best was the Naive Bayes Classifier."
      ],
      "metadata": {
        "id": "LTcLt_WHZw48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Analysis**\n",
        "\n",
        "# Dataset Information\n",
        "\n",
        "The dataset includes 19 attributes and 1 target (as defined by the dataset author), with 5,950 rows in total.\n",
        "\n",
        "The class that is predicted from the attributes is \"confidence_label,\" which can contains the class labels:\n",
        "1. confident\n",
        "2. neutral\n",
        "3. low\n",
        "\n",
        "</br>\n",
        "\n",
        "Although there are 19 attributes used to classify confidence_label, only 9 attributes were used in the different classifier approaches. The 9 attributes were chosen as they were the only attributes that could be changed based on confidence level. Certain attributes in the dataset, such as \"eye_distance,\" for example, which represents the distance between eyes, do not change if a person is confident or not, and so only attributes that were deemed to be possible predictors for confidence_label were used. These attributes include:\n",
        "1. \"body_lean_x\"\n",
        "    * A float value representing the horizontal body lean ratio\n",
        "1. \"shoulder_center_x\"\n",
        "    * A float value representing the X-coordinate of the shoulder center\n",
        "1. \"hip_center_x\"\n",
        "    * A float value representing the X-coordinate of the hip-center\n",
        "1. \"spine_angle\"\n",
        "    * A float value representing the spine inclination angle in degrees\n",
        "1. \"head_tilt_angle\"\n",
        "    * A float value representing the head tilt angle in degrees\n",
        "1. \"shoulder_slope\"\n",
        "    * A float value representing the slope of the shoulder line\n",
        "1. \"head_direction\"\n",
        "    * A categorical value representing the head orientation (can be \"Looking Straight,\" \"Center,\" \"Looking Right,\" or \"Looking Left\")\n",
        "1. \"arm_position\"\n",
        "    * A categorical value representing the arm position (can be \"Partially Open,\" \"Closed Arms,\" or \"Open Arms\")\n",
        "1. \"posture\"\n",
        "    * A categorical vaue representing the general body posture (can be \"Upright,\" \"Stiff,\" or \"Slouched).\n"
      ],
      "metadata": {
        "id": "B32RH-FyaILD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading\n",
        "\n",
        "Below, the dataset is extracted and saved as a MyPyTable object. A copy is then created, as all the float values will undergo normalization and discretization for algorithm compatibility for all classifiers. The copy will ensure the original dataset's values are not changed, retaining its true values.\n",
        "\n",
        "\n",
        "The copy of the dataset then undergoes normalization, ensuring all values are between [0, 1]. After being normalized, the values then undergo discretization, wherein every 0.1 (normalized) value is set to a value between [1, 10]. This ensures algorithms such as Decision Trees have a limited number of attribute values, preventing every unique float value creating a branch in the tree."
      ],
      "metadata": {
        "id": "xdlSI92jwBsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_raw_data = MyPyTable().load_from_file(\"confidence_features.csv\") # dataset before processing\n",
        "confidence = confidence_raw_data.new_deep_copy() # the dataset we will normalize. Original dataset retains its true values\n",
        "\n",
        "all_attributes = [\"body_lean_x\", \"shoulder_center_x\", \"hip_center_x\", \"spine_angle\", \"head_tilt_angle\", \"shoulder_slope\", \"head_direction\", \"arm_position\", \"posture\"]\n",
        "continuous_attributes = all_attributes[:6]\n",
        "categorical_attributes = all_attributes[6:]"
      ],
      "metadata": {
        "id": "NzIzGrRswBX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relevant Summary Statistics\n",
        "\n",
        "\n",
        "In Figure 1 below, the distribution of class labels in the dataset are visualized, with over half being classified as \"Confident,\" and the class with the smallest percentage being \"Low,\" making up only 19.4%."
      ],
      "metadata": {
        "id": "cshMcHqrcZLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pie chart of the percentages of each confidence label in the dataset\n",
        "\n",
        "# reset figure\n",
        "plt.figure(figsize = (4, 4))\n",
        "\n",
        "# get x and y values (frequency of each confidence label)\n",
        "freq = myutils.get_frequency(confidence_raw_data.get_column(\"confidence_label\"))\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "\n",
        "for key in freq:\n",
        "    xs.append(key)\n",
        "    ys.append(freq[key])\n",
        "\n",
        "# create the chart (with number of decimals)\n",
        "plt.pie(ys, labels = xs, autopct = \"%1.1f%%\")\n",
        "\n",
        "# add border to current figure\n",
        "fig = plt.gcf()\n",
        "\n",
        "# add title to pie chart, and change formatting\n",
        "fig.suptitle(\"Figure 1: Confidence Label Percentages\")\n",
        "fig.patch.set_edgecolor(\"black\")\n",
        "fig.patch.set_linewidth(1)\n",
        "\n",
        "plt.show"
      ],
      "metadata": {
        "id": "USyJbo3qhxni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, Figures 2, 3, and 4 show the life cycle of our data preprocessing, with Figure 2 showing a histogram, representing the distribution of the raw, unchanged data containing the true values from the original dataset."
      ],
      "metadata": {
        "id": "Ej60qPof2Ho3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of the body_lean_x attribute before preprocessing\n",
        "\n",
        "# reset figure\n",
        "plt.figure()\n",
        "\n",
        "# get x values\n",
        "x = confidence_raw_data.get_column(\"body_lean_x\")\n",
        "\n",
        "plt.hist(x, bins = 20, alpha = 0.75)\n",
        "plt.grid(True)\n",
        "\n",
        "# add titles\n",
        "plt.title(\"Figure 2: body_lean_x (before preprocessing)\")\n",
        "plt.xlabel(\"Horizontal body lean ratio\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7tZP20RdvMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data then undergoes normalization, as shown in Figure 3. Figure 3 shows a histogram, representing the distribution of the dataset after being normalized to the values between [0, 1]."
      ],
      "metadata": {
        "id": "SagQwfWp2099"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of the body_lean_x attribute after normalization\n",
        "\n",
        "# reset figure\n",
        "plt.figure()\n",
        "\n",
        "# get x values after normalization\n",
        "x = confidence.get_column(\"body_lean_x\")\n",
        "\n",
        "plt.hist(x, bins = 20, alpha = 0.75)\n",
        "plt.grid(True)\n",
        "\n",
        "# add titles\n",
        "plt.title(\"Figure 3: Body Lean X (after normalization)\")\n",
        "plt.xlabel(\"Horizontal body lean ratio\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CVxGb19fv2-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After being normalized, the values are then discretization, where every 0.1 value is set to a value between [1, 10], meaning values between [0, 0.1] are set to 1, values between (0.1, 0.2] are set to 2, etc."
      ],
      "metadata": {
        "id": "5LFuL3Qv0L6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the column indices of the attributes we're using\n",
        "att_indices = []\n",
        "for att in all_attributes:\n",
        "   att_idx = confidence.column_names.index(att)\n",
        "   att_indices.append(att_idx)\n",
        "\n",
        "# discretize values (to make continuous attribute vals --> categorical attribute vals)\n",
        "for row_index, row in enumerate(confidence.data):\n",
        "    for val_index, value in enumerate(row):\n",
        "        if val_index in att_indices and type(confidence.data[row_index][val_index]) != str:\n",
        "            confidence.data[row_index][val_index] = myutils.my_discretizer(confidence.data[row_index][val_index])"
      ],
      "metadata": {
        "id": "H67-t0pk0w_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the normalized and discretized values are shown in Figure 4, representing the distribution of values for the body_lean_X attribute.\n",
        "\n",
        "This normalization and discretization is done to all float values that are attributes, ensuring consistency across all numeric values for future classification."
      ],
      "metadata": {
        "id": "cCcXoC1L03FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of the body_lean_x attribute after discretization\n",
        "\n",
        "# reset figure\n",
        "plt.figure()\n",
        "\n",
        "# get x values after normalization AND discretization\n",
        "x = confidence.get_column(\"body_lean_x\")\n",
        "\n",
        "plt.hist(x, bins = 20, alpha = 0.75)\n",
        "plt.grid(True)\n",
        "\n",
        "# add titles\n",
        "plt.title(\"Figure 4: Body Lean X (after discretization)\")\n",
        "plt.xlabel(\"Horizontal body lean ratio\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vCKicm4K0LcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Results**\n",
        "\n",
        "The 3 different classification approaches that were developed were: Decision Tree classifier, Random Forest classifier, and Naive Bayes classifier.\n",
        "\n",
        "Note: the Decision Tree classifier and Naive Bayes classifier use the k-fold cross validation approach to create training and testing sets. For both classifiers, k = 10."
      ],
      "metadata": {
        "id": "iUpanKA-4HTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Decision Tree for Classification\n",
        "\n",
        "Below, the Decision Tree classification is fitted, and then used to classify unseen instances. After, various performance metrics are used to verify the performance of the Decision Tree classifier.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Generating Test and Training Sets**\n",
        "\n",
        "For the Decision Tree classifier, the training and test sets are created using the k-fold cross validation approach, which partitions the dataset into k (approximately) equal subsets/folds. Then, training and testing is completed k times, wherein each fold becomes the test set one time.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Fitting the Classifier**\n",
        "\n",
        "After k sets of training and testing sets are created, a decision tree is fitted for each training set. Each decision tree is created by using the Top Down Induction of Decision Trees Algorithm. This algorithm selects an attribute to split the data based on having the lowest entropy (meaning the least uncertainty). Each partition is then repeatedly partitioned until either all class values for a partion is the same, there are no more attributes to split on, or there are no more instances to partition. In all cases, a leaf node is created.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Predicting Unseen Instances**\n",
        "\n",
        "After a decision tree is created for a fold, the testing set is then used against the tree to predict its class label. This is done by traversing down the tree, based on the attribute value at each node."
      ],
      "metadata": {
        "id": "O5Itod4VAuga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define X and y data\n",
        "X = confidence.get_column_rows(all_attributes)\n",
        "y = confidence.get_column(\"confidence_label\")\n",
        "\n",
        "# get all unique class labels.\n",
        "labels = list(set(y))\n",
        "\n",
        "# compute k fold cross validation with k=10 folds to evaluate model performance across different train and test subsets of data.\n",
        "acc, err_rate, precision, recall, f1, y_trues, y_preds = myutils.cross_val_predict(X, y, 10, MyDecisionTreeClassifier, True)"
      ],
      "metadata": {
        "id": "WTyoZXrAE_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Performance**\n",
        "\n",
        "For each fold in the dataset, after the decision tree is fitted and tested against, the performance metrics for each fold is found. After all k folds fit and test against decision trees, the average for each performance metric is found, and outputted below.\n",
        "\n",
        "The performance metrics include:\n",
        "1. Accuracy\n",
        "1. Error rate\n",
        "1. Precision\n",
        "1. Recall\n",
        "1. F1 Score\n",
        "\n",
        "The results of each performance metric is outputted below"
      ],
      "metadata": {
        "id": "lso4i-bKFJnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- PERFORMANCE METRICS FOR DECISION TREE -----\")\n",
        "print(f\"Accuracy Score: {round(acc, 2)}\")\n",
        "print(f\"Error Rate: {round(err_rate, 2)}\")\n",
        "print(f\"Precision Score: {round(precision, 2)}\")\n",
        "print(f\"Recall Score: {round(recall, 2)}\")\n",
        "print(f\"F1 Score: {round(f1, 2)}\")"
      ],
      "metadata": {
        "id": "O44fejbWFNQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Random Forest for Classification\n",
        "\n",
        "Below, the Random Forest classifier is fitted, and then used to classify unseen instances. After, various performance metrics are used to verify the performance of the Random Forest classifier.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Generating Test and Training Sets**\n",
        "\n",
        "For the Random Forest classifier, a pre-processing step occurs to the given dataset, wherein both training and testing sets are generated. This is done by first generating a random stratified test set, consisting of one third of the original dataset, with the test set having the same class distribution as the original dataset. After the test set is created, any remaining rows in the dataset that were not selected make up the training set for the classifier, which is then used to fit the classifier.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Fitting the Classifier**\n",
        "\n",
        "Fitting the classifier is done by creating N \"random\" decision trees using bootstrapping (a technique where random rows are selected with replacement, making up the training set. Any values not in the training set make up the test set) over the remainder set. At each node when creating the tree, F random attributes are selected as candidate attributes to partition data on (an attribute is still chosen based on entropy, similar to the Decision Tree classifier). After all N trees are created, the M most accurate decision trees, with accuracy being found based on the training and testing sets found using boostrapping, out of the N trees, are selected.\n",
        "\n",
        "For this dataset, after testing against multiple N, F, and M values, it was determined the values did not change the performance of the Forest by much, so the Random Forest for the confidence level dataset is set to:\n",
        "1. N = 20\n",
        "1. M = 5\n",
        "1. F = 4"
      ],
      "metadata": {
        "id": "QdRhdvsW4cyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define X and y data\n",
        "X = confidence.get_column_rows(all_attributes)\n",
        "y = confidence.get_column(\"confidence_label\")\n",
        "\n",
        "# create a random forest classifer instance using the best N, M, and F parameters found.\n",
        "myForest = MyRandomForestClassifier(N=20, M=5, F=4)\n",
        "\n",
        "# train the random forest classifier on our train data. (class does internal split into train and test set, so here we just use internal train set).\n",
        "myForest.fit(X, y)"
      ],
      "metadata": {
        "id": "3dFcGXoj87gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Unseen Instances**\n",
        "\n",
        "After the Random Forest classifier is fitted, the testing sets found earlier are then used to against the classifier. This is done by running each unseen instance against all M trees. The class label predicted the most out of the M trees is then considered the predicted class for that unseen attribute."
      ],
      "metadata": {
        "id": "1mP4sX7f88SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = myForest.predict()"
      ],
      "metadata": {
        "id": "pMON0P8n-5zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Performance**\n",
        "\n",
        "After the class predictions are found against the unseen instances, various performance metrics are used against the classifier to verify its performance.\n",
        "\n",
        "\n",
        "The results of each performance metric is ouputted below."
      ],
      "metadata": {
        "id": "kC9eFlNK93-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- PERFORMANCE METRICS FOR RANDOM FOREST -----\")\n",
        "print(f\"Accuracy Score: {round(myevaluation.accuracy_score(myForest.y_test, y_preds), 2)}\")\n",
        "print(f\"Error Rate: {round(1 - myevaluation.accuracy_score(myForest.y_test, y_preds), 2)}\")\n",
        "print(f\"Precision Score: {round(myevaluation.multiclass_precision_score(myForest.y_test, y_preds, [\"Confident\", \"Neutral\", \"Low\"]), 2)}\")\n",
        "print(f\"Recall Score: {round(myevaluation.multiclass_recall_score(myForest.y_test, y_preds, [\"Confident\", \"Neutral\", \"Low\"]), 2)}\")\n",
        "print(f\"F1 Score: {round(myevaluation.multiclass_f1_score(myForest.y_test, y_preds, [\"Confident\", \"Neutral\", \"Low\"]), 2)}\")"
      ],
      "metadata": {
        "id": "2JeMxAcK_led"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}